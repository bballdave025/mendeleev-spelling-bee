# language-resources

This is a place to put dictionaries from multiple languages. Other
resources for NLP/Linguistics will be put here as needed. It is
foreseen that this folder will exist for multiple projects here on
GitHub. I'm going to grab some stuff from other projects, then
collect them here.

This first folder where I'm including the README is for the
`mendeleev-spelling-bee` repository. Here, a search such as the one
used in the old BYU Boggle project will be used to find the different
combinations of Mendeleev Chart (Periodic Table) element symbols that
create words existing in the dictionaries for different languages.

@TODO : put in descriptions of the resources and how they might be used.

## MS Word Documents

The MS Word documents are copy/paste from some site that has a bunch of
linguistic stuff - perhaps just word lists. From a quick look, they
come from the University of Leipzig. Looking further, the first link
with a language name goes to 
https://wortschatz.uni-leipzig.de/en/download/Achinese
Which has the following text. (I'm taking out what I think is important.)

> Download Corpora Achinese <br/>
> <br/>
> To download a corpus select a corpus size - given in number of 
sentences - and download the corresponding data file. <br/>
> <br/>
> 2017     \[little download icon\] Alle

There's also a link, 
[Go back to main download site](https://wortschatz.uni-leipzig.de/en/download)
which goes to https://wortschatz.uni-leipzig.de/en/download
There, we find some more info.

> The Leipzig Corpora Collection presents corpora in different languages 
> using the same format and comparable sources. All data are available as 
> plain text files and can be imported into a MySQL database by using the 
> provided import script. They are intended both for scientific use by 
> corpus linguists as well as for applications such as knowledge 
> extraction programs. <br/>
> The corpora are identical in format and similar in size and content. 
> They contain randomly selected sentences in the language of the corpus 
> and are available in sizes from 10,000 sentences up to 1 million 
> sentences. The sources are either newspaper texts or texts randomly 
> collected from the web. The texts are split into sentences. 
> Non-sentences and foreign language material was removed. Because word 
> co-occurrence information is useful for many applications, these data 
> are precomputed and included as well. For each word, the most 
> significant words appearing as immediate left or right neighbor or 
> appearing anywhere within the same sentence are given. More information 
> about the format and content of these files can be found 
>[here](https://wortschatz.uni-leipzig.de/public/documents/Format_Download_File-eng.pdf).<br/>
> The corpora are automatically collected from carefully selected public 
> sources without considering in detail the content of the contained 
> text. No responsibility is taken for the content of the data. In 
> particular, the views and opinions expressed in specific parts of the 
> data remain exclusively with the authors. <br/>
> <br/>
> If you use one of these corpora in your work we kindly ask you to cite 
> [this paper](http://www.lrec-conf.org/proceedings/lrec2012/pdf/327_Paper.pdf) 
> as <br/>
> <br/>
> D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual 
> Dictionaries at the Leipzig Corpora Collection: From 100 to 200 
> Languages. <br/>
> In: <i>Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012</i> <br/>